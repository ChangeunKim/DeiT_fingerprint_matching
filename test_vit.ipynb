{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cCQExteXKcs"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install pillow\n",
        "!pip install kagglehub\n",
        "!pip install scikit-learn\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login()"
      ],
      "metadata": {
        "id": "OZoPoIa43uT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "executionInfo": {
          "elapsed": 249,
          "status": "ok",
          "timestamp": 1730979539341,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "YBK5SmPGXKcu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from PIL import Image\n",
        "from itertools import combinations\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.amp import GradScaler, autocast\n",
        "import timm\n",
        "from torchvision import models, transforms\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1730979541444,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "mlCtdOvvXKcu"
      },
      "outputs": [],
      "source": [
        "def create_image_pairs_labels(dataset_dir, num_impostor_pairs=1000):\n",
        "    # Dictionary to organize images by user and finger type\n",
        "    user_finger_dict = {}\n",
        "\n",
        "    # Step 1: Parse filenames and organize by user ID and finger type\n",
        "    for filename in os.listdir(dataset_dir):\n",
        "        if filename.endswith(\".BMP\"):\n",
        "            parts = filename.split(\"__\")  # Splitting by double underscore\n",
        "            user_num = parts[0]\n",
        "            finger_type = parts[1].rsplit(\"_\", 1)[0]  # Extracting finger info without augmentation part\n",
        "\n",
        "            # Add to dictionary\n",
        "            if (user_num, finger_type) not in user_finger_dict:\n",
        "                user_finger_dict[(user_num, finger_type)] = []\n",
        "            user_finger_dict[(user_num, finger_type)].append(os.path.join(dataset_dir, filename))\n",
        "\n",
        "    image_pairs = []\n",
        "    labels = []\n",
        "\n",
        "    # Step 2: Create genuine pairs (same user, same finger)\n",
        "    for _, images in user_finger_dict.items():\n",
        "        if len(images) > 1:\n",
        "            for img1, img2 in combinations(images, 2):\n",
        "                image_pairs.append((img1, img2))\n",
        "                labels.append(1)  # 1 for genuine pairs\n",
        "\n",
        "    # Step 3: Create impostor pairs (different user or different finger)\n",
        "    user_finger_keys = list(user_finger_dict.keys())\n",
        "    num_pairs = 0\n",
        "\n",
        "    while num_pairs < num_impostor_pairs:\n",
        "        user_finger1, user_finger2 = random.sample(user_finger_keys, 2)\n",
        "\n",
        "        # Ensure different user or finger type\n",
        "        if user_finger1[0] != user_finger2[0] or user_finger1[1] != user_finger2[1]:\n",
        "            img1 = random.choice(user_finger_dict[user_finger1])\n",
        "            img2 = random.choice(user_finger_dict[user_finger2])\n",
        "            image_pairs.append((img1, img2))\n",
        "            labels.append(-1)  # -1 for impostor pairs\n",
        "            num_pairs += 1\n",
        "\n",
        "    return image_pairs, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transforms:\n",
        "    def __init__(self, augmentation=True):\n",
        "        # Define augmentation transforms (Albumentations)\n",
        "        self.augmentation_transforms = A.Compose([\n",
        "            A.RandomBrightnessContrast(p=0.2),\n",
        "            # Flipping to account for possible orientation changes\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            # Gaussian blur to simulate minor blurring in fingerprints\n",
        "            A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
        "            # Crop and resize to the original transformer input size\n",
        "            A.RandomResizedCrop(height=244, width=244, scale=(0.8, 1.0), p=0.5),\n",
        "            # Shift, scale, and rotate to account for finger misplacement\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
        "            # Random occlusion to simulate partial visibility of fingerprints\n",
        "            A.CoarseDropout(max_holes=5, max_height=20, max_width=20, min_holes=1, min_height=10, min_width=10, fill_value=0, p=0.4),\n",
        "\n",
        "        ]) if augmentation else None\n",
        "\n",
        "        # Define resize and normalization (Albumentations and PyTorch)\n",
        "        self.resize_and_normalize = A.Compose([\n",
        "            A.Resize(224, 224),  # Resizing to 224x224\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "            ToTensorV2()  # Convert to PyTorch tensor\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert PIL Image to numpy array if necessary\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        # Apply augmentation transforms if enabled\n",
        "        if self.augmentation_transforms:\n",
        "            img = self.augmentation_transforms(image=img)[\"image\"]\n",
        "\n",
        "        # Apply resize and normalize transforms\n",
        "        img = self.resize_and_normalize(image=img)[\"image\"]\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "avilXV2WzSjG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730979541695,
          "user_tz": -540,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "executionInfo": {
          "elapsed": 329,
          "status": "ok",
          "timestamp": 1730979542485,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "ek3xW400XKcv"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset for Siamese Network\n",
        "class SiameseFingerprintDataset(Dataset):\n",
        "    def __init__(self, image_pairs, labels, transform=None):\n",
        "        self.image_pairs = image_pairs\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load images from file paths\n",
        "        img1_path, img2_path = self.image_pairs[index]\n",
        "        if not os.path.isfile(img1_path):\n",
        "            raise FileNotFoundError(f\"Image 1 not found: {img1_path}\")\n",
        "        if not os.path.isfile(img2_path):\n",
        "            raise FileNotFoundError(f\"Image 2 not found: {img2_path}\")\n",
        "\n",
        "        img1 = Image.open(img1_path).convert(\"RGB\")  # Ensure 3 channels\n",
        "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply transformations\n",
        "        img1 = self.transform(img1)\n",
        "        img2 = self.transform(img2)\n",
        "\n",
        "        # Get label\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
        "\n",
        "        return img1, img2, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1730979543149,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "NWqbII0xXKcv"
      },
      "outputs": [],
      "source": [
        "class SiameseDeiTTiny(nn.Module):\n",
        "    def __init__(self, embedding = 64):\n",
        "        super(SiameseDeiTTiny, self).__init__()\n",
        "        self.deit = timm.create_model('deit_tiny_patch16_224', pretrained=True)\n",
        "\n",
        "        # Get the number of features from the previous layer\n",
        "        num_features = self.deit.head.in_features\n",
        "\n",
        "        # Remove the original classification head\n",
        "        self.deit.head = None\n",
        "\n",
        "        # Add a new fully connected layer with the desired embedding dimension\n",
        "        self.deit.head = nn.Linear(num_features, embedding)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        embed1 = self.deit(img1)\n",
        "        embed2 = self.deit(img2)\n",
        "        return embed1, embed2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapper function to return the corresponding distance metric function\n",
        "def get_distance_metric_function(distance_metric):\n",
        "    \"\"\"\n",
        "    Returns the appropriate distance metric function based on the string input.\n",
        "\n",
        "    :param distance_metric: One of ['euclidean', 'cosine', 'manhattan', 'mahalanobis', 'dot', 'squared_euclidean']\n",
        "    :return: Corresponding distance metric function\n",
        "    \"\"\"\n",
        "    if distance_metric == 'euclidean':\n",
        "        return euclidean_distance\n",
        "    elif distance_metric == 'cosine':\n",
        "        return cosine_similarity\n",
        "    elif distance_metric == 'manhattan':\n",
        "        return manhattan_distance\n",
        "    elif distance_metric == 'mahalanobis':\n",
        "        return mahalanobis_distance\n",
        "    elif distance_metric == 'dot':\n",
        "        return dot_product\n",
        "    elif distance_metric == 'squared_euclidean':\n",
        "        return squared_euclidean_distance\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown distance metric: {distance_metric}\")\n",
        "\n",
        "# Define individual distance functions\n",
        "def euclidean_distance(output1, output2):\n",
        "    return F.pairwise_distance(output1, output2, keepdim=True)\n",
        "\n",
        "def cosine_similarity(output1, output2):\n",
        "    return 1 - F.cosine_similarity(output1, output2)\n",
        "\n",
        "def manhattan_distance(output1, output2):\n",
        "    return torch.sum(torch.abs(output1 - output2), dim=1, keepdim=True)\n",
        "\n",
        "def mahalanobis_distance(output1, output2):\n",
        "    diff = output1 - output2\n",
        "    return torch.sqrt(torch.sum(torch.pow(diff, 2), dim=1, keepdim=True))\n",
        "\n",
        "def dot_product(output1, output2):\n",
        "    return torch.sum(output1 * output2, dim=1, keepdim=True)\n",
        "\n",
        "def squared_euclidean_distance(output1, output2):\n",
        "    return torch.sum(torch.pow(output1 - output2, 2), dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0, distance_metric_fn=None):\n",
        "        \"\"\"\n",
        "        Constructor for ContrastiveLoss.\n",
        "\n",
        "        :param margin: Margin for Contrastive Loss. Default is 1.0.\n",
        "        :param distance_metric_fn: A function for distance metric (e.g., euclidean_distance, cosine_similarity, etc.)\n",
        "        \"\"\"\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.distance_metric_fn = distance_metric_fn\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        # Calculate distance using the passed distance metric function\n",
        "        distance = self.distance_metric_fn(output1, output2)\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        loss = torch.mean((label == 1) * torch.pow(distance, 2) +\n",
        "                          (label == -1) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "sZK-36hGBtqV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730979543149,
          "user_tz": -540,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1730979544671,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "XkM5JBrfhOBT"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def calculate_far_frr(predictions, labels):\n",
        "    # Convert inputs to numpy arrays for easier boolean indexing\n",
        "    predictions = np.array(predictions)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Calculate True Positives, False Positives, True Negatives, and False Negatives\n",
        "    true_positives = ((predictions == 1) & (labels == 1)).sum()  # Genuine pairs correctly identified\n",
        "    false_positives = ((predictions == 1) & (labels == -1)).sum()  # Imposter pairs incorrectly identified as genuine\n",
        "    true_negatives = ((predictions == -1) & (labels == -1)).sum()  # Imposter pairs correctly identified\n",
        "    false_negatives = ((predictions == -1) & (labels == 1)).sum()  # Genuine pairs incorrectly identified as imposter\n",
        "\n",
        "    # Calculate FAR and FRR\n",
        "    far = false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0\n",
        "    frr = false_negatives / (false_negatives + true_positives) if (false_negatives + true_positives) > 0 else 0\n",
        "\n",
        "    return far, frr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1730979544671,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "CCRfe9_eZAI0"
      },
      "outputs": [],
      "source": [
        "# Training Function\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, patience=5):\n",
        "    model.train()\n",
        "\n",
        "    # Initialize variables for early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # GradScaler for mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc='Epoch: '):\n",
        "        model.train()  # Set model to training mode\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for img1, img2, label in train_loader:\n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with autocast(device_type='cuda', dtype=torch.float16):\n",
        "                embed1, embed2 = model(img1, img2)\n",
        "                loss = criterion(embed1, embed2, label.float())\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for img1, img2, label in val_loader:\n",
        "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                with autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    embed1, embed2 = model(img1, img2)\n",
        "                    val_loss = criterion(embed1, embed2, label.float())\n",
        "                    total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the best model state\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # If the patience counter exceeds the patience threshold, stop training\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered. Restoring best model state.\")\n",
        "            model.load_state_dict(best_model_state)\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Testing Function\n",
        "def test_model(model, test_loader, device, distance_metric_fn=None):\n",
        "    model.eval()\n",
        "    embeddings1 = []\n",
        "    embeddings2 = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in  test_loader:\n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            embed1, embed2 = model(img1, img2)\n",
        "            embeddings1.append(embed1)\n",
        "            embeddings2.append(embed2)\n",
        "            labels.extend(label.cpu().numpy())\n",
        "\n",
        "    embeddings1 = torch.cat(embeddings1)\n",
        "    embeddings2 = torch.cat(embeddings2)\n",
        "\n",
        "    similarities = distance_metric_fn(embeddings1, embeddings2)\n",
        "\n",
        "    return labels, similarities\n",
        "\n",
        "def evaluate_result(labels, similarities, threshold):\n",
        "    # Label the predicted cosine similarity depending on the threshold\n",
        "    predictions = np.where(similarities.cpu().numpy() <= threshold, 1, -1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    far, frr = calculate_far_frr(predictions, labels)\n",
        "\n",
        "    return accuracy, far, frr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1137,
          "status": "ok",
          "timestamp": 1730979546065,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "rHZIlrOKXKcv",
        "outputId": "24daf817-134d-43c0-daa1-31b3ef8168e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ruizgara/socofing/versions/2\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ruizgara/socofing\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1730979546065,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "8Ku0srpdXKcv"
      },
      "outputs": [],
      "source": [
        "path_easy = os.path.join(path, 'SOCOFing/Altered/Altered-Easy')\n",
        "path_medium = os.path.join(path, 'SOCOFing/Altered/Altered-Medium')\n",
        "path_hard = os.path.join(path, 'SOCOFing/Altered/Altered-Hard')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "executionInfo": {
          "elapsed": 1532,
          "status": "ok",
          "timestamp": 1730979547596,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "cqk6PVrkXKcw"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset\n",
        "hard_image_pairs, hard_labels = create_image_pairs_labels(path_hard, num_impostor_pairs=30000)\n",
        "medium_image_pairs, medium_labels = create_image_pairs_labels(path_medium, num_impostor_pairs=30000)\n",
        "easy_image_pairs, easy_labels = create_image_pairs_labels(path_easy, num_impostor_pairs=30000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_pairs = hard_image_pairs + medium_image_pairs + easy_image_pairs\n",
        "labels = hard_labels + medium_labels + easy_labels\n",
        "\n",
        "combined = list(zip(image_pairs, labels))\n",
        "\n",
        "# Shuffle the combined array\n",
        "np.random.shuffle(combined)\n",
        "\n",
        "# Unzip the shuffled array back into image pairs and labels\n",
        "image_pairs, labels = zip(*combined)"
      ],
      "metadata": {
        "id": "EtAB67I5oDFI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730979551762,
          "user_tz": -540,
          "elapsed": 819,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "executionInfo": {
          "elapsed": 307,
          "status": "ok",
          "timestamp": 1730979554019,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "-NPgX0rVdPkT"
      },
      "outputs": [],
      "source": [
        "# Initial split: 80% for training/validation and 20% for testing\n",
        "train_val_size = int(0.8 * len(image_pairs))\n",
        "test_size = len(image_pairs) - train_val_size\n",
        "\n",
        "train_val_image_pairs, test_image_pairs = image_pairs[:train_val_size], image_pairs[train_val_size:]\n",
        "train_val_labels, test_labels = labels[:train_val_size], labels[train_val_size:]\n",
        "\n",
        "# Further split the training/validation set into 90% training and 10% validation\n",
        "train_size = int(0.9 * len(train_val_image_pairs))\n",
        "val_size = len(train_val_image_pairs) - train_size\n",
        "\n",
        "train_image_pairs, val_image_pairs = train_val_image_pairs[:train_size], train_val_image_pairs[train_size:]\n",
        "train_labels, val_labels = train_val_labels[:train_size], train_val_labels[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "7PBVSUCqXKcw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730979554253,
          "user_tz": -540,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SiameseFingerprintDataset(train_image_pairs, train_labels, Transforms(augmentation=True))\n",
        "val_dataset = SiameseFingerprintDataset(val_image_pairs, val_labels, Transforms(augmentation=False))\n",
        "test_dataset = SiameseFingerprintDataset(test_image_pairs, test_labels, Transforms(augmentation=False))\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning\n",
        "num_epochs = 300\n",
        "patience = 10\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Loss function\n",
        "margin = 1.2\n",
        "distance_metric = 'mahalanobis' # distance_metric = ['euclidean', 'cosine', 'manhattan', 'mahalanobis', 'dot', 'squared_euclidean']\n",
        "distance_fn = get_distance_metric_function(distance_metric)"
      ],
      "metadata": {
        "id": "1JqH9Nw1fU99",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730979556435,
          "user_tz": -540,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "AEADNFmbXKcw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730979557517,
          "user_tz": -540,
          "elapsed": 850,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        " # Initialize Model, Loss, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiameseDeiTTiny().to(device)\n",
        "criterion = ContrastiveLoss(margin=margin, distance_metric_fn=distance_fn)\n",
        "#criterion = nn.CosineEmbeddingLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emqU7voGXKcw"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Starting Training...\")\n",
        "model = train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
        "torch.save(model, 'model_cr_loss_mahalanobis.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "aborted",
          "timestamp": 1730980025220,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "jJqw1UIJXKcw"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "print(\"Testing the model...\")\n",
        "model = torch.load('model_cr_loss_mahalanobis.pt', map_location=device, weights_only=False)\n",
        "labels, similarities = test_model(model, test_loader, device, distance_metric_fn=distance_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMS6hmhGIvxW",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1730980025220,
          "user_tz": -540,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define a range of thresholds to test\n",
        "thresholds = np.linspace(0, 1, 100)  # Adjust the range and step size as needed\n",
        "\n",
        "# Lists to store the metrics for each threshold\n",
        "accuracies = []\n",
        "fars = []\n",
        "frrs = []\n",
        "\n",
        "# Loop through each threshold, calculate metrics, and store the results\n",
        "for threshold in thresholds:\n",
        "    accuracy, far, frr = evaluate_result(labels, similarities, threshold)\n",
        "    accuracies.append(accuracy)\n",
        "    fars.append(far)\n",
        "    frrs.append(frr)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, np.array(accuracies) * 100, label=\"Accuracy (%)\", color=\"blue\")\n",
        "plt.plot(thresholds, np.array(fars) * 100, label=\"FAR (%)\", color=\"red\")\n",
        "plt.plot(thresholds, np.array(frrs) * 100, label=\"FRR (%)\", color=\"green\")\n",
        "\n",
        "# Adding labels and legend\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.title(\"Accuracy, FAR, and FRR vs. Threshold\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "xs4A4Zx7C6IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XC_yYmOtzdYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "test_vit.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}